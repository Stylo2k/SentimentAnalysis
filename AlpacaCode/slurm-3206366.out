cuda
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:13,  2.41it/s]Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:10,  2.97it/s]Loading checkpoint shards:   9%|▉         | 3/33 [00:00<00:09,  3.14it/s]Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:08,  3.32it/s]Loading checkpoint shards:  15%|█▌        | 5/33 [00:01<00:08,  3.37it/s]Loading checkpoint shards:  18%|█▊        | 6/33 [00:01<00:07,  3.41it/s]Loading checkpoint shards:  21%|██        | 7/33 [00:02<00:07,  3.42it/s]Loading checkpoint shards:  24%|██▍       | 8/33 [00:02<00:07,  3.48it/s]Loading checkpoint shards:  27%|██▋       | 9/33 [00:02<00:06,  3.47it/s]Loading checkpoint shards:  30%|███       | 10/33 [00:02<00:06,  3.47it/s]Loading checkpoint shards:  33%|███▎      | 11/33 [00:03<00:06,  3.48it/s]Loading checkpoint shards:  36%|███▋      | 12/33 [00:03<00:06,  3.46it/s]Loading checkpoint shards:  39%|███▉      | 13/33 [00:03<00:05,  3.46it/s]Loading checkpoint shards:  42%|████▏     | 14/33 [00:04<00:05,  3.47it/s]Loading checkpoint shards:  45%|████▌     | 15/33 [00:04<00:05,  3.47it/s]Loading checkpoint shards:  48%|████▊     | 16/33 [00:04<00:04,  3.51it/s]Loading checkpoint shards:  52%|█████▏    | 17/33 [00:04<00:04,  3.50it/s]Loading checkpoint shards:  55%|█████▍    | 18/33 [00:05<00:04,  3.46it/s]Loading checkpoint shards:  58%|█████▊    | 19/33 [00:05<00:04,  3.40it/s]Loading checkpoint shards:  61%|██████    | 20/33 [00:05<00:03,  3.33it/s]Loading checkpoint shards:  64%|██████▎   | 21/33 [00:06<00:03,  3.24it/s]Loading checkpoint shards:  67%|██████▋   | 22/33 [00:06<00:03,  3.32it/s]Loading checkpoint shards:  70%|██████▉   | 23/33 [00:06<00:02,  3.37it/s]Loading checkpoint shards:  73%|███████▎  | 24/33 [00:07<00:02,  3.41it/s]Loading checkpoint shards:  76%|███████▌  | 25/33 [00:07<00:02,  3.45it/s]Loading checkpoint shards:  79%|███████▉  | 26/33 [00:07<00:02,  3.45it/s]Loading checkpoint shards:  82%|████████▏ | 27/33 [00:07<00:01,  3.46it/s]Loading checkpoint shards:  85%|████████▍ | 28/33 [00:08<00:01,  3.47it/s]Loading checkpoint shards:  88%|████████▊ | 29/33 [00:08<00:01,  3.44it/s]Loading checkpoint shards:  91%|█████████ | 30/33 [00:08<00:00,  3.45it/s]Loading checkpoint shards:  94%|█████████▍| 31/33 [00:09<00:00,  3.43it/s]Loading checkpoint shards:  97%|█████████▋| 32/33 [00:09<00:00,  3.49it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.35it/s]Loading checkpoint shards: 100%|██████████| 33/33 [00:09<00:00,  3.39it/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Found cached dataset json (/home3/p306726/.cache/huggingface/datasets/json/default-d57cd3b38718e8dd/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 21.31it/s]
Loading cached split indices for dataset at /home3/p306726/.cache/huggingface/datasets/json/default-d57cd3b38718e8dd/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-d6b90e9746f03750.arrow and /home3/p306726/.cache/huggingface/datasets/json/default-d57cd3b38718e8dd/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-786ebb0d07592546.arrow
Dataset({
    features: ['input', 'output', 'instruction'],
    num_rows: 538
})
Map:   0%|          | 0/338 [00:00<?, ? examples/s]Map:   0%|          | 1/338 [00:00<01:27,  3.84 examples/s]Map:  37%|███▋      | 124/338 [00:00<00:00, 437.06 examples/s]Map:  79%|███████▊  | 266/338 [00:00<00:00, 765.24 examples/s]                                                              Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:  30%|██▉       | 59/200 [00:00<00:00, 505.16 examples/s]Map: 100%|██████████| 200/200 [00:00<00:00, 933.49 examples/s]                                                              /scratch/p306726/alpaca_lora/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
Traceback (most recent call last):
  File "/home3/p306726/stanford_alpaca/own_train.py", line 148, in <module>
    trainer = transformers.Trainer(
  File "/scratch/p306726/alpaca_lora/lib/python3.10/site-packages/transformers/trainer.py", line 541, in __init__
    self.callback_handler = CallbackHandler(
  File "/scratch/p306726/alpaca_lora/lib/python3.10/site-packages/transformers/trainer_callback.py", line 296, in __init__
    self.add_callback(cb)
  File "/scratch/p306726/alpaca_lora/lib/python3.10/site-packages/transformers/trainer_callback.py", line 313, in add_callback
    cb = callback() if isinstance(callback, type) else callback
  File "/scratch/p306726/alpaca_lora/lib/python3.10/site-packages/transformers/integrations.py", line 585, in __init__
    raise RuntimeError(
RuntimeError: TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or install tensorboardX.

###############################################################################
Hábrók Cluster
Job 3206366 for user 'p306726'
Finished at: Sun Jul 23 23:34:33 CEST 2023

Job details:
============

Job ID              : 3206366
Name                : fine_tune.sh
User                : p306726
Partition           : gpushort
Nodes               : a100gpu6
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : FAILED
Submit              : 2023-07-23T23:27:19
Start               : 2023-07-23T23:29:07
End                 : 2023-07-23T23:34:33
Reserved walltime   : 04:00:00
Used walltime       : 00:05:26
Used CPU time       : 00:00:35 (efficiency: 10.84%)
% User (Computation): 61.62%
% System (I/O)      : 38.37%
Mem reserved        : 160G
Max Mem (Node/step) : 1.44G (a100gpu6, per node)
Full Max Mem usage  : 1.44G
Total Disk Read     : 1021.09K
Total Disk Write    : 42.42K

Acknowledgements:
=================

Please see this page for information about acknowledging Hábrók in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
