Requirement already satisfied: pip in ./ownenv/lib/python3.10/site-packages (23.2)
Collecting git+https://github.com/huggingface/peft.git (from -r install.sh (line 10))
  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-rubvdbo3
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-rubvdbo3
  Resolved https://github.com/huggingface/peft.git to commit 30fd5a4c88db369e87eab27ebf01c0b28bed02dc
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting git+https://github.com/huggingface/transformers.git (from -r install.sh (line 11))
  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-3kt6t6b1
  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-3kt6t6b1
  Resolved https://github.com/huggingface/transformers.git to commit 129cb6d5239374869beea0e1317641426e8df572
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: accelerate==0.20.3 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 1)) (0.20.3)
Requirement already satisfied: appdirs==1.4.4 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 2)) (1.4.4)
Collecting bitsandbytes==0.37.2 (from -r install.sh (line 3))
  Using cached bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)
Requirement already satisfied: datasets==2.10.1 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 4)) (2.10.1)
Requirement already satisfied: fire==0.5.0 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 5)) (0.5.0)
Requirement already satisfied: torch==2.0.0 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 6)) (2.0.0)
Requirement already satisfied: sentencepiece==0.1.97 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 7)) (0.1.97)
Requirement already satisfied: tensorboardX==2.6 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 8)) (2.6)
Requirement already satisfied: gradio==3.23.0 in ./ownenv/lib/python3.10/site-packages (from -r install.sh (line 9)) (3.23.0)
Requirement already satisfied: numpy>=1.17 in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3->-r install.sh (line 1)) (1.25.1)
Requirement already satisfied: packaging>=20.0 in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3->-r install.sh (line 1)) (23.1)
Requirement already satisfied: psutil in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3->-r install.sh (line 1)) (5.9.5)
Requirement already satisfied: pyyaml in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3->-r install.sh (line 1)) (6.0.1)
Requirement already satisfied: pyarrow>=6.0.0 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (12.0.1)
Requirement already satisfied: dill<0.3.7,>=0.3.0 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (0.3.6)
Requirement already satisfied: pandas in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (2.0.3)
Requirement already satisfied: requests>=2.19.0 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (2.31.0)
Requirement already satisfied: tqdm>=4.62.1 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (4.65.0)
Requirement already satisfied: xxhash in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (3.2.0)
Requirement already satisfied: multiprocess in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (0.70.14)
Requirement already satisfied: fsspec[http]>=2021.11.1 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (2023.6.0)
Requirement already satisfied: aiohttp in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (3.8.4)
Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (0.16.4)
Requirement already satisfied: responses<0.19 in ./ownenv/lib/python3.10/site-packages (from datasets==2.10.1->-r install.sh (line 4)) (0.18.0)
Requirement already satisfied: six in ./ownenv/lib/python3.10/site-packages (from fire==0.5.0->-r install.sh (line 5)) (1.16.0)
Requirement already satisfied: termcolor in ./ownenv/lib/python3.10/site-packages (from fire==0.5.0->-r install.sh (line 5)) (2.3.0)
Requirement already satisfied: filelock in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (3.12.2)
Requirement already satisfied: typing-extensions in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (4.7.1)
Requirement already satisfied: sympy in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (1.12)
Requirement already satisfied: networkx in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (3.1)
Requirement already satisfied: jinja2 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (3.1.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.7.99)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.7.99)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.7.101)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (8.5.0.96)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.10.3.66)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (10.9.0.58)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (10.2.10.91)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.4.0.1)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.7.4.91)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (2.14.3)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (11.7.91)
Requirement already satisfied: triton==2.0.0 in ./ownenv/lib/python3.10/site-packages (from torch==2.0.0->-r install.sh (line 6)) (2.0.0)
Requirement already satisfied: protobuf<4,>=3.8.0 in ./ownenv/lib/python3.10/site-packages (from tensorboardX==2.6->-r install.sh (line 8)) (3.20.3)
Requirement already satisfied: aiofiles in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (23.1.0)
Requirement already satisfied: altair>=4.2.0 in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (5.0.1)
Requirement already satisfied: fastapi in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.100.0)
Requirement already satisfied: ffmpy in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.3.1)
Requirement already satisfied: httpx in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.24.1)
Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (2.2.0)
Requirement already satisfied: markupsafe in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (2.1.3)
Requirement already satisfied: matplotlib in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (3.7.2)
Requirement already satisfied: mdit-py-plugins<=0.3.3 in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.3.3)
Requirement already satisfied: orjson in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (3.9.2)
Requirement already satisfied: pillow in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (10.0.0)
Requirement already satisfied: pydantic in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (2.0.3)
Requirement already satisfied: pydub in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.25.1)
Requirement already satisfied: python-multipart in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.0.6)
Requirement already satisfied: semantic-version in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (2.10.0)
Requirement already satisfied: uvicorn in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (0.23.1)
Requirement already satisfied: websockets>=10.0 in ./ownenv/lib/python3.10/site-packages (from gradio==3.23.0->-r install.sh (line 9)) (11.0.3)
Requirement already satisfied: setuptools in ./ownenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r install.sh (line 6)) (68.0.0)
Requirement already satisfied: wheel in ./ownenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r install.sh (line 6)) (0.40.0)
Requirement already satisfied: cmake in ./ownenv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->-r install.sh (line 6)) (3.26.4)
Requirement already satisfied: lit in ./ownenv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->-r install.sh (line 6)) (16.0.6)
Requirement already satisfied: safetensors in ./ownenv/lib/python3.10/site-packages (from peft==0.5.0.dev0->-r install.sh (line 10)) (0.3.1)
Requirement already satisfied: regex!=2019.12.17 in ./ownenv/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r install.sh (line 11)) (2023.6.3)
Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./ownenv/lib/python3.10/site-packages (from transformers==4.32.0.dev0->-r install.sh (line 11)) (0.13.3)
Requirement already satisfied: jsonschema>=3.0 in ./ownenv/lib/python3.10/site-packages (from altair>=4.2.0->gradio==3.23.0->-r install.sh (line 9)) (4.18.4)
Requirement already satisfied: toolz in ./ownenv/lib/python3.10/site-packages (from altair>=4.2.0->gradio==3.23.0->-r install.sh (line 9)) (0.12.0)
Requirement already satisfied: attrs>=17.3.0 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (23.1.0)
Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (3.2.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (6.0.4)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (4.0.2)
Requirement already satisfied: yarl<2.0,>=1.0 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (1.4.0)
Requirement already satisfied: aiosignal>=1.1.2 in ./ownenv/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r install.sh (line 4)) (1.3.1)
Requirement already satisfied: mdurl~=0.1 in ./ownenv/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0->-r install.sh (line 9)) (0.1.2)
Requirement already satisfied: linkify-it-py<3,>=1 in ./ownenv/lib/python3.10/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.23.0->-r install.sh (line 9)) (2.0.2)
Requirement already satisfied: python-dateutil>=2.8.2 in ./ownenv/lib/python3.10/site-packages (from pandas->datasets==2.10.1->-r install.sh (line 4)) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in ./ownenv/lib/python3.10/site-packages (from pandas->datasets==2.10.1->-r install.sh (line 4)) (2023.3)
Requirement already satisfied: tzdata>=2022.1 in ./ownenv/lib/python3.10/site-packages (from pandas->datasets==2.10.1->-r install.sh (line 4)) (2023.3)
Requirement already satisfied: idna<4,>=2.5 in ./ownenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1->-r install.sh (line 4)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./ownenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1->-r install.sh (line 4)) (2.0.3)
Requirement already satisfied: certifi>=2017.4.17 in ./ownenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.10.1->-r install.sh (line 4)) (2023.5.7)
Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./ownenv/lib/python3.10/site-packages (from fastapi->gradio==3.23.0->-r install.sh (line 9)) (0.27.0)
Requirement already satisfied: annotated-types>=0.4.0 in ./ownenv/lib/python3.10/site-packages (from pydantic->gradio==3.23.0->-r install.sh (line 9)) (0.5.0)
Requirement already satisfied: pydantic-core==2.3.0 in ./ownenv/lib/python3.10/site-packages (from pydantic->gradio==3.23.0->-r install.sh (line 9)) (2.3.0)
Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in ./ownenv/lib/python3.10/site-packages (from httpx->gradio==3.23.0->-r install.sh (line 9)) (0.17.3)
Requirement already satisfied: sniffio in ./ownenv/lib/python3.10/site-packages (from httpx->gradio==3.23.0->-r install.sh (line 9)) (1.3.0)
Requirement already satisfied: contourpy>=1.0.1 in ./ownenv/lib/python3.10/site-packages (from matplotlib->gradio==3.23.0->-r install.sh (line 9)) (1.1.0)
Requirement already satisfied: cycler>=0.10 in ./ownenv/lib/python3.10/site-packages (from matplotlib->gradio==3.23.0->-r install.sh (line 9)) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in ./ownenv/lib/python3.10/site-packages (from matplotlib->gradio==3.23.0->-r install.sh (line 9)) (4.41.0)
Requirement already satisfied: kiwisolver>=1.0.1 in ./ownenv/lib/python3.10/site-packages (from matplotlib->gradio==3.23.0->-r install.sh (line 9)) (1.4.4)
Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./ownenv/lib/python3.10/site-packages (from matplotlib->gradio==3.23.0->-r install.sh (line 9)) (3.0.9)
Requirement already satisfied: mpmath>=0.19 in ./ownenv/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r install.sh (line 6)) (1.3.0)
Requirement already satisfied: click>=7.0 in ./ownenv/lib/python3.10/site-packages (from uvicorn->gradio==3.23.0->-r install.sh (line 9)) (8.1.6)
Requirement already satisfied: h11>=0.8 in ./ownenv/lib/python3.10/site-packages (from uvicorn->gradio==3.23.0->-r install.sh (line 9)) (0.14.0)
Requirement already satisfied: anyio<5.0,>=3.0 in ./ownenv/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.23.0->-r install.sh (line 9)) (3.7.1)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./ownenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23.0->-r install.sh (line 9)) (2023.7.1)
Requirement already satisfied: referencing>=0.28.4 in ./ownenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23.0->-r install.sh (line 9)) (0.30.0)
Requirement already satisfied: rpds-py>=0.7.1 in ./ownenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.23.0->-r install.sh (line 9)) (0.9.2)
Requirement already satisfied: uc-micro-py in ./ownenv/lib/python3.10/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.23.0->-r install.sh (line 9)) (1.0.2)
Requirement already satisfied: exceptiongroup in ./ownenv/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.23.0->-r install.sh (line 9)) (1.1.2)
Installing collected packages: bitsandbytes
  Attempting uninstall: bitsandbytes
    Found existing installation: bitsandbytes 0.40.2
    Uninstalling bitsandbytes-0.40.2:
      Successfully uninstalled bitsandbytes-0.40.2
Successfully installed bitsandbytes-0.37.2
removing bitsandbytes
bitsandbytes
bitsandbytes-0.37.2.dist-info
installing bitsandbytes
Collecting bitsandbytes
  Obtaining dependency information for bitsandbytes from https://files.pythonhosted.org/packages/32/ea/75961538b08e4ed568057198717aabdebeaf6f398b7692420532e6861754/bitsandbytes-0.40.2-py3-none-any.whl.metadata
  Using cached bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)
Using cached bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)
Installing collected packages: bitsandbytes
Successfully installed bitsandbytes-0.40.2
bitsandbytes
bitsandbytes-0.40.2.dist-info
Requirement already satisfied: accelerate==0.20.3 in ./ownenv/lib/python3.10/site-packages (0.20.3)
Requirement already satisfied: numpy>=1.17 in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3) (1.25.1)
Requirement already satisfied: packaging>=20.0 in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3) (23.1)
Requirement already satisfied: psutil in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.5)
Requirement already satisfied: pyyaml in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3) (6.0.1)
Requirement already satisfied: torch>=1.6.0 in ./ownenv/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.0)
Requirement already satisfied: filelock in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.12.2)
Requirement already satisfied: typing-extensions in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.7.1)
Requirement already satisfied: sympy in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)
Requirement already satisfied: networkx in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)
Requirement already satisfied: jinja2 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)
Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.99)
Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.99)
Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.101)
Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (8.5.0.96)
Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.10.3.66)
Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (10.9.0.58)
Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (10.2.10.91)
Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.4.0.1)
Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.4.91)
Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (2.14.3)
Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (11.7.91)
Requirement already satisfied: triton==2.0.0 in ./ownenv/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (2.0.0)
Requirement already satisfied: setuptools in ./ownenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.20.3) (68.0.0)
Requirement already satisfied: wheel in ./ownenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate==0.20.3) (0.40.0)
Requirement already satisfied: cmake in ./ownenv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.3) (3.26.4)
Requirement already satisfied: lit in ./ownenv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.3) (16.0.6)
Requirement already satisfied: MarkupSafe>=2.0 in ./ownenv/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)
Requirement already satisfied: mpmath>=0.19 in ./ownenv/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)
Requirement already satisfied: scipy in ./ownenv/lib/python3.10/site-packages (1.11.1)
Requirement already satisfied: numpy<1.28.0,>=1.21.6 in ./ownenv/lib/python3.10/site-packages (from scipy) (1.25.1)
==============================================
Running own_train.py
The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.
cuda
Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]Loading checkpoint shards:   3%|â–Ž         | 1/33 [00:00<00:07,  4.33it/s]Loading checkpoint shards:   6%|â–Œ         | 2/33 [00:00<00:06,  4.47it/s]Loading checkpoint shards:   9%|â–‰         | 3/33 [00:00<00:06,  4.43it/s]Loading checkpoint shards:  12%|â–ˆâ–        | 4/33 [00:00<00:06,  4.49it/s]Loading checkpoint shards:  15%|â–ˆâ–Œ        | 5/33 [00:01<00:06,  4.55it/s]Loading checkpoint shards:  18%|â–ˆâ–Š        | 6/33 [00:01<00:05,  4.58it/s]Loading checkpoint shards:  21%|â–ˆâ–ˆ        | 7/33 [00:01<00:05,  4.59it/s]Loading checkpoint shards:  24%|â–ˆâ–ˆâ–       | 8/33 [00:01<00:05,  4.61it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 9/33 [00:01<00:05,  4.62it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 10/33 [00:02<00:04,  4.61it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 11/33 [00:02<00:04,  4.63it/s]Loading checkpoint shards:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 12/33 [00:02<00:04,  4.63it/s]Loading checkpoint shards:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 13/33 [00:02<00:04,  4.62it/s]Loading checkpoint shards:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/33 [00:03<00:04,  4.63it/s]Loading checkpoint shards:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 15/33 [00:03<00:03,  4.63it/s]Loading checkpoint shards:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16/33 [00:03<00:03,  4.62it/s]Loading checkpoint shards:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 17/33 [00:03<00:03,  4.62it/s]Loading checkpoint shards:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/33 [00:03<00:03,  4.63it/s]Loading checkpoint shards:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 19/33 [00:04<00:03,  4.63it/s]Loading checkpoint shards:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20/33 [00:04<00:02,  4.63it/s]Loading checkpoint shards:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 21/33 [00:04<00:02,  4.63it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 22/33 [00:04<00:02,  4.51it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 23/33 [00:05<00:02,  4.56it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 24/33 [00:05<00:01,  4.60it/s]Loading checkpoint shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 25/33 [00:05<00:01,  4.61it/s]Loading checkpoint shards:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 26/33 [00:05<00:01,  4.58it/s]Loading checkpoint shards:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/33 [00:05<00:01,  4.61it/s]Loading checkpoint shards:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 28/33 [00:06<00:01,  4.63it/s]Loading checkpoint shards:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 29/33 [00:06<00:00,  4.65it/s]Loading checkpoint shards:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 30/33 [00:06<00:00,  4.66it/s]Loading checkpoint shards:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 31/33 [00:06<00:00,  4.67it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 32/33 [00:06<00:00,  4.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.39it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.58it/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. 
The class this function is called from is 'LlamaTokenizer'.
You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Downloading and preparing dataset json/default to /home3/p306726/.cache/huggingface/datasets/json/default-701dbb27269d1cc1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 10591.68it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 409.28it/s]
Generating train split: 0 examples [00:00, ? examples/s]                                                        Dataset json downloaded and prepared to /home3/p306726/.cache/huggingface/datasets/json/default-701dbb27269d1cc1/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 451.92it/s]
Dataset({
    features: ['instruction', 'input', 'output'],
    num_rows: 538
})
Map:   0%|          | 0/338 [00:00<?, ? examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 176/338 [00:00<00:00, 1738.54 examples/s]                                                               Map:   0%|          | 0/200 [00:00<?, ? examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [00:00<00:00, 1805.99 examples/s]                                                               /home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.
  warnings.warn(
trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199
  0%|          | 0/300 [00:00<?, ?it/s]/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|          | 1/300 [00:38<3:11:44, 38.48s/it]  1%|          | 2/300 [01:12<2:58:49, 36.01s/it]  1%|          | 3/300 [01:48<2:57:10, 35.79s/it]  1%|â–         | 4/300 [02:25<2:59:15, 36.34s/it]  2%|â–         | 5/300 [03:00<2:56:27, 35.89s/it]  2%|â–         | 6/300 [03:35<2:53:29, 35.41s/it]  2%|â–         | 7/300 [04:09<2:51:50, 35.19s/it]  3%|â–Ž         | 8/300 [04:46<2:53:42, 35.69s/it]  3%|â–Ž         | 9/300 [05:22<2:52:58, 35.67s/it]  3%|â–Ž         | 10/300 [05:58<2:53:50, 35.97s/it]                                                    3%|â–Ž         | 10/300 [05:58<2:53:50, 35.97s/it]  4%|â–Ž         | 11/300 [06:34<2:53:06, 35.94s/it]  4%|â–         | 12/300 [07:10<2:51:59, 35.83s/it]  4%|â–         | 13/300 [07:46<2:51:55, 35.94s/it]  5%|â–         | 14/300 [08:21<2:50:41, 35.81s/it]  5%|â–Œ         | 15/300 [08:57<2:49:08, 35.61s/it]  5%|â–Œ         | 16/300 [09:34<2:50:45, 36.07s/it]  6%|â–Œ         | 17/300 [10:12<2:53:07, 36.71s/it]  6%|â–Œ         | 18/300 [10:47<2:49:41, 36.10s/it]  6%|â–‹         | 19/300 [11:21<2:46:24, 35.53s/it]  7%|â–‹         | 20/300 [11:57<2:46:50, 35.75s/it]                                                    7%|â–‹         | 20/300 [11:57<2:46:50, 35.75s/it]  7%|â–‹         | 21/300 [12:34<2:48:23, 36.21s/it]  7%|â–‹         | 22/300 [13:09<2:45:10, 35.65s/it]  8%|â–Š         | 23/300 [13:46<2:46:23, 36.04s/it]  8%|â–Š         | 24/300 [14:19<2:42:36, 35.35s/it]  8%|â–Š         | 25/300 [14:54<2:41:12, 35.17s/it]  9%|â–Š         | 26/300 [15:31<2:43:19, 35.76s/it]  9%|â–‰         | 27/300 [16:05<2:39:50, 35.13s/it]  9%|â–‰         | 28/300 [16:41<2:40:23, 35.38s/it] 10%|â–‰         | 29/300 [17:17<2:40:20, 35.50s/it] 10%|â–ˆ         | 30/300 [17:54<2:42:42, 36.16s/it]                                                   10%|â–ˆ         | 30/300 [17:54<2:42:42, 36.16s/it] 10%|â–ˆ         | 31/300 [18:29<2:39:37, 35.60s/it] 11%|â–ˆ         | 32/300 [19:04<2:38:37, 35.51s/it] 11%|â–ˆ         | 33/300 [19:40<2:38:25, 35.60s/it] 11%|â–ˆâ–        | 34/300 [20:18<2:40:47, 36.27s/it] 12%|â–ˆâ–        | 35/300 [20:53<2:39:32, 36.12s/it] 12%|â–ˆâ–        | 36/300 [21:29<2:37:40, 35.83s/it] 12%|â–ˆâ–        | 37/300 [22:05<2:37:15, 35.88s/it] 13%|â–ˆâ–Ž        | 38/300 [22:40<2:36:24, 35.82s/it] 13%|â–ˆâ–Ž        | 39/300 [23:15<2:34:20, 35.48s/it] 13%|â–ˆâ–Ž        | 40/300 [23:52<2:35:44, 35.94s/it]                                                   13%|â–ˆâ–Ž        | 40/300 [23:52<2:35:44, 35.94s/it] 14%|â–ˆâ–Ž        | 41/300 [24:29<2:36:04, 36.15s/it] 14%|â–ˆâ–        | 42/300 [25:04<2:34:02, 35.82s/it] 14%|â–ˆâ–        | 43/300 [25:38<2:31:41, 35.42s/it] 15%|â–ˆâ–        | 44/300 [26:15<2:33:11, 35.90s/it] 15%|â–ˆâ–Œ        | 45/300 [26:51<2:32:03, 35.78s/it] 15%|â–ˆâ–Œ        | 46/300 [27:28<2:33:15, 36.20s/it] 16%|â–ˆâ–Œ        | 47/300 [28:03<2:31:38, 35.96s/it] 16%|â–ˆâ–Œ        | 48/300 [28:36<2:26:53, 34.97s/it] 16%|â–ˆâ–‹        | 49/300 [29:13<2:29:22, 35.71s/it] 17%|â–ˆâ–‹        | 50/300 [29:49<2:28:24, 35.62s/it]                                                   17%|â–ˆâ–‹        | 50/300 [29:49<2:28:24, 35.62s/it]{'loss': 3.4349, 'learning_rate': 2.9999999999999997e-05, 'epoch': 3.76}
{'loss': 3.2589, 'learning_rate': 5.9999999999999995e-05, 'epoch': 7.53}
{'loss': 2.8684, 'learning_rate': 8.699999999999999e-05, 'epoch': 11.29}
{'loss': 2.1112, 'learning_rate': 0.00011399999999999999, 'epoch': 15.06}
{'loss': 1.5231, 'learning_rate': 0.00014399999999999998, 'epoch': 18.82}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.53it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.86it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.11it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.46it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.17it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.18it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.49it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.29it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.12it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.14it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.30it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.19it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.02it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.90it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.83it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  2.98it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.89it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.82it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.05it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.23it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.29it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.38it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.31it/s][A                                                  
                                               [A 17%|â–ˆâ–‹        | 50/300 [29:57<2:28:24, 35.62s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.31it/s][A
                                               [A/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 17%|â–ˆâ–‹        | 51/300 [30:31<2:36:33, 37.72s/it] 17%|â–ˆâ–‹        | 52/300 [31:08<2:34:14, 37.31s/it] 18%|â–ˆâ–Š        | 53/300 [31:43<2:30:40, 36.60s/it] 18%|â–ˆâ–Š        | 54/300 [32:19<2:29:35, 36.49s/it] 18%|â–ˆâ–Š        | 55/300 [32:53<2:26:11, 35.80s/it] 19%|â–ˆâ–Š        | 56/300 [33:29<2:25:22, 35.75s/it] 19%|â–ˆâ–‰        | 57/300 [34:05<2:25:22, 35.89s/it] 19%|â–ˆâ–‰        | 58/300 [34:39<2:23:05, 35.48s/it] 20%|â–ˆâ–‰        | 59/300 [35:17<2:24:24, 35.95s/it] 20%|â–ˆâ–ˆ        | 60/300 [35:53<2:24:24, 36.10s/it]                                                   20%|â–ˆâ–ˆ        | 60/300 [35:53<2:24:24, 36.10s/it] 20%|â–ˆâ–ˆ        | 61/300 [36:28<2:22:02, 35.66s/it] 21%|â–ˆâ–ˆ        | 62/300 [37:02<2:20:24, 35.40s/it] 21%|â–ˆâ–ˆ        | 63/300 [37:38<2:20:06, 35.47s/it] 21%|â–ˆâ–ˆâ–       | 64/300 [38:14<2:20:02, 35.60s/it] 22%|â–ˆâ–ˆâ–       | 65/300 [38:49<2:18:14, 35.30s/it] 22%|â–ˆâ–ˆâ–       | 66/300 [39:26<2:19:57, 35.89s/it] 22%|â–ˆâ–ˆâ–       | 67/300 [40:01<2:19:09, 35.84s/it] 23%|â–ˆâ–ˆâ–Ž       | 68/300 [40:37<2:18:31, 35.82s/it] 23%|â–ˆâ–ˆâ–Ž       | 69/300 [41:14<2:18:53, 36.07s/it] 23%|â–ˆâ–ˆâ–Ž       | 70/300 [41:49<2:16:48, 35.69s/it]                                                   23%|â–ˆâ–ˆâ–Ž       | 70/300 [41:49<2:16:48, 35.69s/it] 24%|â–ˆâ–ˆâ–Ž       | 71/300 [42:23<2:14:39, 35.28s/it] 24%|â–ˆâ–ˆâ–       | 72/300 [42:59<2:14:52, 35.49s/it] 24%|â–ˆâ–ˆâ–       | 73/300 [43:35<2:15:05, 35.71s/it] 25%|â–ˆâ–ˆâ–       | 74/300 [44:11<2:14:13, 35.64s/it] 25%|â–ˆâ–ˆâ–Œ       | 75/300 [44:46<2:13:13, 35.53s/it] 25%|â–ˆâ–ˆâ–Œ       | 76/300 [45:20<2:10:27, 34.94s/it] 26%|â–ˆâ–ˆâ–Œ       | 77/300 [45:56<2:11:11, 35.30s/it] 26%|â–ˆâ–ˆâ–Œ       | 78/300 [46:29<2:08:07, 34.63s/it] 26%|â–ˆâ–ˆâ–‹       | 79/300 [47:05<2:08:46, 34.96s/it] 27%|â–ˆâ–ˆâ–‹       | 80/300 [47:40<2:08:15, 34.98s/it]                                                   27%|â–ˆâ–ˆâ–‹       | 80/300 [47:40<2:08:15, 34.98s/it] 27%|â–ˆâ–ˆâ–‹       | 81/300 [48:16<2:09:03, 35.36s/it] 27%|â–ˆâ–ˆâ–‹       | 82/300 [48:51<2:08:01, 35.24s/it] 28%|â–ˆâ–ˆâ–Š       | 83/300 [49:27<2:08:15, 35.46s/it] 28%|â–ˆâ–ˆâ–Š       | 84/300 [50:02<2:07:04, 35.30s/it] 28%|â–ˆâ–ˆâ–Š       | 85/300 [50:38<2:07:43, 35.65s/it] 29%|â–ˆâ–ˆâ–Š       | 86/300 [51:13<2:06:24, 35.44s/it] 29%|â–ˆâ–ˆâ–‰       | 87/300 [51:47<2:04:28, 35.06s/it] 29%|â–ˆâ–ˆâ–‰       | 88/300 [52:25<2:06:22, 35.77s/it] 30%|â–ˆâ–ˆâ–‰       | 89/300 [52:59<2:03:49, 35.21s/it] 30%|â–ˆâ–ˆâ–ˆ       | 90/300 [53:34<2:03:08, 35.18s/it]                                                   30%|â–ˆâ–ˆâ–ˆ       | 90/300 [53:34<2:03:08, 35.18s/it] 30%|â–ˆâ–ˆâ–ˆ       | 91/300 [54:09<2:03:08, 35.35s/it] 31%|â–ˆâ–ˆâ–ˆ       | 92/300 [54:45<2:02:59, 35.48s/it] 31%|â–ˆâ–ˆâ–ˆ       | 93/300 [55:21<2:02:37, 35.54s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 94/300 [55:58<2:03:30, 35.98s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 95/300 [56:34<2:03:20, 36.10s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 96/300 [57:09<2:01:11, 35.65s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 97/300 [57:44<2:00:00, 35.47s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 98/300 [58:21<2:00:45, 35.87s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 99/300 [58:56<1:59:21, 35.63s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [59:33<2:00:45, 36.23s/it]                                                    33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [59:33<2:00:45, 36.23s/it]{'eval_loss': 1.455501675605774, 'eval_runtime': 8.012, 'eval_samples_per_second': 24.962, 'eval_steps_per_second': 3.12, 'epoch': 18.82}
{'loss': 1.2802, 'learning_rate': 0.00017399999999999997, 'epoch': 22.59}
{'loss': 1.1577, 'learning_rate': 0.000204, 'epoch': 26.35}
{'loss': 1.1077, 'learning_rate': 0.000234, 'epoch': 30.12}
{'loss': 1.0774, 'learning_rate': 0.00026399999999999997, 'epoch': 33.88}
{'loss': 1.0419, 'learning_rate': 0.000294, 'epoch': 37.65}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.50it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.86it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.11it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.48it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.17it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.19it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.50it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.30it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.12it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.15it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.31it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.19it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.03it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.90it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.84it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  2.99it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.89it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.82it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.05it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.24it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.30it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.39it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.65it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.31it/s][A                                                   
                                               [A 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 100/300 [59:41<2:00:45, 36.23s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.31it/s][A
                                               [A/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 101/300 [1:00:14<2:04:39, 37.59s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 102/300 [1:00:49<2:01:26, 36.80s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 103/300 [1:01:26<2:01:19, 36.95s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 104/300 [1:02:03<2:00:21, 36.84s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 105/300 [1:02:39<1:58:29, 36.46s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 106/300 [1:03:14<1:57:06, 36.22s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 107/300 [1:03:49<1:55:08, 35.79s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 108/300 [1:04:24<1:53:46, 35.55s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 109/300 [1:05:00<1:53:23, 35.62s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [1:05:35<1:52:26, 35.51s/it]                                                      37%|â–ˆâ–ˆâ–ˆâ–‹      | 110/300 [1:05:35<1:52:26, 35.51s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 111/300 [1:06:12<1:53:38, 36.08s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 112/300 [1:06:48<1:52:39, 35.96s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 113/300 [1:07:24<1:51:42, 35.84s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 114/300 [1:07:58<1:49:24, 35.29s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 115/300 [1:08:32<1:48:01, 35.03s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 116/300 [1:09:09<1:48:43, 35.45s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 117/300 [1:09:44<1:48:02, 35.42s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 118/300 [1:10:19<1:47:26, 35.42s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 119/300 [1:10:55<1:47:02, 35.48s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [1:11:32<1:47:50, 35.95s/it]                                                      40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 120/300 [1:11:32<1:47:50, 35.95s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 121/300 [1:12:07<1:46:44, 35.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 122/300 [1:12:42<1:45:27, 35.55s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 123/300 [1:13:18<1:44:37, 35.46s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 124/300 [1:13:53<1:44:12, 35.52s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 125/300 [1:14:28<1:43:03, 35.34s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 126/300 [1:15:06<1:44:16, 35.96s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 127/300 [1:15:40<1:42:26, 35.53s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 128/300 [1:16:16<1:41:56, 35.56s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 129/300 [1:16:51<1:41:07, 35.48s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 130/300 [1:17:26<1:40:22, 35.43s/it]                                                      43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 130/300 [1:17:26<1:40:22, 35.43s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 131/300 [1:18:01<1:38:44, 35.06s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 132/300 [1:18:38<1:39:54, 35.68s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 133/300 [1:19:14<1:39:59, 35.92s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 134/300 [1:19:49<1:38:41, 35.67s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 135/300 [1:20:25<1:37:57, 35.62s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 136/300 [1:21:01<1:37:41, 35.74s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 137/300 [1:21:35<1:35:47, 35.26s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 138/300 [1:22:11<1:35:55, 35.53s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 139/300 [1:22:45<1:34:12, 35.11s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 140/300 [1:23:23<1:35:26, 35.79s/it]                                                      47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 140/300 [1:23:23<1:35:26, 35.79s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 141/300 [1:23:58<1:34:45, 35.76s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 142/300 [1:24:34<1:33:53, 35.66s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 143/300 [1:25:09<1:32:49, 35.48s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 144/300 [1:25:44<1:32:16, 35.49s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 145/300 [1:26:20<1:32:06, 35.66s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 146/300 [1:26:56<1:31:37, 35.70s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 147/300 [1:27:31<1:30:06, 35.34s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 148/300 [1:28:06<1:29:48, 35.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 149/300 [1:28:41<1:28:22, 35.11s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 150/300 [1:29:16<1:27:33, 35.02s/it]                                                      50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 150/300 [1:29:16<1:27:33, 35.02s/it]{'eval_loss': 1.1768059730529785, 'eval_runtime': 7.999, 'eval_samples_per_second': 25.003, 'eval_steps_per_second': 3.125, 'epoch': 37.65}
{'loss': 0.9607, 'learning_rate': 0.00028799999999999995, 'epoch': 41.41}
{'loss': 0.9098, 'learning_rate': 0.00027299999999999997, 'epoch': 45.18}
{'loss': 0.8341, 'learning_rate': 0.000258, 'epoch': 48.94}
{'loss': 0.7675, 'learning_rate': 0.000243, 'epoch': 52.71}
{'loss': 0.6801, 'learning_rate': 0.00022799999999999999, 'epoch': 56.47}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.52it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.88it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.14it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.49it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.19it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.20it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.51it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.30it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.12it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.15it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.32it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.20it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.04it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.91it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.85it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.00it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.90it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.83it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.06it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.24it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.31it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.40it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.66it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.32it/s][A                                                     
                                               [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 150/300 [1:29:24<1:27:33, 35.02s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.32it/s][A
                                               [A/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 151/300 [1:30:00<1:33:47, 37.77s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 152/300 [1:30:35<1:31:11, 36.97s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 153/300 [1:31:10<1:29:29, 36.53s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 154/300 [1:31:47<1:28:46, 36.49s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 155/300 [1:32:22<1:27:38, 36.26s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 156/300 [1:32:56<1:25:23, 35.58s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 157/300 [1:33:32<1:24:42, 35.54s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 158/300 [1:34:04<1:21:56, 34.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 159/300 [1:34:42<1:23:12, 35.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 160/300 [1:35:16<1:21:37, 34.98s/it]                                                      53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 160/300 [1:35:16<1:21:37, 34.98s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 161/300 [1:35:52<1:22:07, 35.45s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 162/300 [1:36:29<1:22:13, 35.75s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 163/300 [1:37:04<1:21:37, 35.75s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/300 [1:37:40<1:20:42, 35.61s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 165/300 [1:38:15<1:19:59, 35.55s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 166/300 [1:38:51<1:19:57, 35.80s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 167/300 [1:39:28<1:19:54, 36.05s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 168/300 [1:40:01<1:17:28, 35.21s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 169/300 [1:40:36<1:16:39, 35.11s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 170/300 [1:41:12<1:16:40, 35.39s/it]                                                      57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 170/300 [1:41:12<1:16:40, 35.39s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 171/300 [1:41:49<1:17:08, 35.88s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 172/300 [1:42:23<1:15:27, 35.37s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 173/300 [1:42:59<1:14:56, 35.41s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 174/300 [1:43:37<1:15:53, 36.14s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 175/300 [1:44:12<1:14:34, 35.80s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 176/300 [1:44:46<1:13:03, 35.35s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 177/300 [1:45:22<1:12:45, 35.49s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 178/300 [1:45:57<1:12:13, 35.52s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 179/300 [1:46:32<1:11:10, 35.30s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 180/300 [1:47:07<1:10:33, 35.28s/it]                                                      60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 180/300 [1:47:07<1:10:33, 35.28s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 181/300 [1:47:44<1:10:29, 35.54s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 182/300 [1:48:20<1:10:23, 35.79s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 183/300 [1:48:55<1:09:25, 35.61s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 184/300 [1:49:29<1:07:59, 35.17s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 185/300 [1:50:03<1:06:41, 34.80s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 186/300 [1:50:40<1:07:12, 35.37s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 187/300 [1:51:14<1:05:42, 34.89s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 188/300 [1:51:50<1:06:06, 35.42s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 189/300 [1:52:25<1:05:16, 35.29s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 190/300 [1:53:01<1:05:04, 35.50s/it]                                                      63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 190/300 [1:53:01<1:05:04, 35.50s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 191/300 [1:53:37<1:04:30, 35.51s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 192/300 [1:54:12<1:03:49, 35.46s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 193/300 [1:54:49<1:04:00, 35.89s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 194/300 [1:55:23<1:02:29, 35.37s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 195/300 [1:55:58<1:01:35, 35.20s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 196/300 [1:56:33<1:00:48, 35.08s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 197/300 [1:57:09<1:00:35, 35.29s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 198/300 [1:57:45<1:00:26, 35.56s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 199/300 [1:58:19<59:08, 35.14s/it]   67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [1:58:54<58:14, 34.95s/it]                                                    67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [1:58:54<58:14, 34.95s/it]{'eval_loss': 1.3039414882659912, 'eval_runtime': 7.9733, 'eval_samples_per_second': 25.084, 'eval_steps_per_second': 3.135, 'epoch': 56.47}
{'loss': 0.5852, 'learning_rate': 0.00021299999999999997, 'epoch': 60.24}
{'loss': 0.5025, 'learning_rate': 0.000198, 'epoch': 64.0}
{'loss': 0.411, 'learning_rate': 0.00018299999999999998, 'epoch': 67.76}
{'loss': 0.3469, 'learning_rate': 0.000168, 'epoch': 71.53}
{'loss': 0.2885, 'learning_rate': 0.00015299999999999998, 'epoch': 75.29}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.51it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.87it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.14it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.49it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.19it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.20it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.51it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.31it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.13it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.16it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.32it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.21it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.04it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.92it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.85it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.00it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.91it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.83it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.06it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.25it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.32it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.40it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.66it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A                                                   
                                               [A 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 200/300 [1:59:01<58:14, 34.95s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A
                                               [A/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 201/300 [1:59:38<1:02:16, 37.75s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 202/300 [2:00:13<1:00:16, 36.90s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 203/300 [2:00:47<58:20, 36.09s/it]   68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 204/300 [2:01:23<57:47, 36.12s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 205/300 [2:01:58<56:40, 35.79s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 206/300 [2:02:34<56:12, 35.88s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 207/300 [2:03:09<54:59, 35.48s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 208/300 [2:03:43<53:46, 35.07s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 209/300 [2:04:19<53:44, 35.44s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [2:04:56<53:41, 35.79s/it]                                                    70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 210/300 [2:04:56<53:41, 35.79s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 211/300 [2:05:30<52:30, 35.40s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 212/300 [2:06:07<52:41, 35.92s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 213/300 [2:06:44<52:12, 36.00s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 214/300 [2:07:20<51:36, 36.01s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 215/300 [2:07:54<50:14, 35.47s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 216/300 [2:08:28<48:59, 34.99s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 217/300 [2:09:01<47:50, 34.58s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 218/300 [2:09:38<47:59, 35.12s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 219/300 [2:10:15<48:13, 35.72s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 220/300 [2:10:50<47:21, 35.52s/it]                                                    73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 220/300 [2:10:50<47:21, 35.52s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 221/300 [2:11:26<46:52, 35.60s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 222/300 [2:12:00<45:37, 35.10s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 223/300 [2:12:36<45:43, 35.63s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 224/300 [2:13:11<44:43, 35.31s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 225/300 [2:13:46<44:02, 35.23s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 226/300 [2:14:21<43:29, 35.27s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 227/300 [2:14:55<42:27, 34.89s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 228/300 [2:15:31<42:10, 35.14s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 229/300 [2:16:04<40:52, 34.54s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 230/300 [2:16:40<40:40, 34.86s/it]                                                    77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 230/300 [2:16:40<40:40, 34.86s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 231/300 [2:17:17<40:49, 35.50s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 232/300 [2:17:53<40:19, 35.57s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 233/300 [2:18:27<39:15, 35.16s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 234/300 [2:19:03<38:56, 35.40s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 235/300 [2:19:38<38:20, 35.40s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 236/300 [2:20:14<37:44, 35.38s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 237/300 [2:20:47<36:33, 34.82s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 238/300 [2:21:24<36:44, 35.56s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 239/300 [2:22:01<36:33, 35.96s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 240/300 [2:22:35<35:13, 35.22s/it]                                                    80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 240/300 [2:22:35<35:13, 35.22s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 241/300 [2:23:11<34:58, 35.57s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 242/300 [2:23:48<34:45, 35.96s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 243/300 [2:24:23<33:49, 35.61s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 244/300 [2:24:58<33:00, 35.36s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 245/300 [2:25:32<32:02, 34.95s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 246/300 [2:26:07<31:30, 35.02s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 247/300 [2:26:45<31:43, 35.91s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 248/300 [2:27:20<30:52, 35.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 249/300 [2:27:53<29:45, 35.00s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 250/300 [2:28:29<29:27, 35.35s/it]                                                    83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 250/300 [2:28:29<29:27, 35.35s/it]{'eval_loss': 1.737331509590149, 'eval_runtime': 7.9607, 'eval_samples_per_second': 25.123, 'eval_steps_per_second': 3.14, 'epoch': 75.29}
{'loss': 0.249, 'learning_rate': 0.000138, 'epoch': 79.06}
{'loss': 0.216, 'learning_rate': 0.00012299999999999998, 'epoch': 82.82}
{'loss': 0.1967, 'learning_rate': 0.00010799999999999998, 'epoch': 86.59}
{'loss': 0.1822, 'learning_rate': 9.3e-05, 'epoch': 90.35}
{'loss': 0.173, 'learning_rate': 7.8e-05, 'epoch': 94.12}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.53it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.89it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.14it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.51it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.19it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.21it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.28it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.12it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.15it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.32it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.21it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.04it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.92it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.85it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.00it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.91it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.83it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.07it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.25it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.32it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.40it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.66it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A                                                   
                                               [A 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 250/300 [2:28:37<29:27, 35.35s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A
                                               [A/home3/p306726/stanford_alpaca/ownenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 251/300 [2:29:13<30:49, 37.74s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 252/300 [2:29:49<29:53, 37.36s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 253/300 [2:30:24<28:41, 36.62s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 254/300 [2:30:58<27:26, 35.80s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 255/300 [2:31:34<26:47, 35.73s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 256/300 [2:32:08<25:56, 35.37s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 257/300 [2:32:46<25:49, 36.03s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 258/300 [2:33:19<24:45, 35.37s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 259/300 [2:33:56<24:23, 35.69s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 260/300 [2:34:29<23:19, 34.99s/it]                                                    87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 260/300 [2:34:29<23:19, 34.99s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 261/300 [2:35:07<23:14, 35.76s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 262/300 [2:35:42<22:27, 35.46s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 263/300 [2:36:16<21:44, 35.25s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 264/300 [2:36:53<21:19, 35.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 265/300 [2:37:28<20:41, 35.46s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 266/300 [2:38:00<19:29, 34.39s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 267/300 [2:38:34<18:55, 34.41s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 268/300 [2:39:10<18:33, 34.80s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 269/300 [2:39:47<18:17, 35.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 270/300 [2:40:21<17:32, 35.08s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 270/300 [2:40:21<17:32, 35.08s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 271/300 [2:40:57<17:03, 35.30s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 272/300 [2:41:32<16:23, 35.13s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 273/300 [2:42:08<15:56, 35.41s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 274/300 [2:42:43<15:16, 35.26s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 275/300 [2:43:18<14:44, 35.39s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 276/300 [2:43:54<14:12, 35.53s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 277/300 [2:44:30<13:37, 35.53s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 278/300 [2:45:05<12:57, 35.34s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 279/300 [2:45:39<12:17, 35.12s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 280/300 [2:46:15<11:48, 35.41s/it]                                                    93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 280/300 [2:46:15<11:48, 35.41s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 281/300 [2:46:51<11:15, 35.56s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 282/300 [2:47:25<10:30, 35.01s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 283/300 [2:48:00<09:53, 34.92s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 284/300 [2:48:35<09:22, 35.15s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 285/300 [2:49:11<08:50, 35.36s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 286/300 [2:49:46<08:14, 35.30s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 287/300 [2:50:21<07:37, 35.21s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 288/300 [2:50:56<07:01, 35.12s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 289/300 [2:51:32<06:28, 35.32s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 290/300 [2:52:07<05:51, 35.20s/it]                                                    97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 290/300 [2:52:07<05:51, 35.20s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 291/300 [2:52:43<05:18, 35.39s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 292/300 [2:53:18<04:43, 35.48s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 293/300 [2:53:54<04:09, 35.57s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 294/300 [2:54:30<03:33, 35.65s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 295/300 [2:55:04<02:55, 35.18s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 296/300 [2:55:41<02:22, 35.60s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 297/300 [2:56:15<01:45, 35.30s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 298/300 [2:56:51<01:10, 35.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 299/300 [2:57:28<00:35, 35.80s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [2:58:04<00:00, 35.95s/it]                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [2:58:04<00:00, 35.95s/it]{'eval_loss': 2.0975587368011475, 'eval_runtime': 7.9596, 'eval_samples_per_second': 25.127, 'eval_steps_per_second': 3.141, 'epoch': 94.12}
{'loss': 0.1656, 'learning_rate': 6.299999999999999e-05, 'epoch': 97.88}
{'loss': 0.1619, 'learning_rate': 4.7999999999999994e-05, 'epoch': 101.65}
{'loss': 0.1586, 'learning_rate': 3.2999999999999996e-05, 'epoch': 105.41}
{'loss': 0.1562, 'learning_rate': 1.7999999999999997e-05, 'epoch': 109.18}
{'loss': 0.1552, 'learning_rate': 2.9999999999999997e-06, 'epoch': 112.94}

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:03,  6.51it/s][A
 12%|â–ˆâ–        | 3/25 [00:00<00:04,  4.88it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:00<00:05,  4.15it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:01<00:05,  3.50it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:01<00:05,  3.19it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:01<00:05,  3.20it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:02<00:04,  3.52it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:02<00:04,  3.31it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:02<00:04,  3.13it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:03<00:04,  3.16it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:03<00:03,  3.33it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:03<00:03,  3.22it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:04<00:03,  3.05it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:04<00:03,  2.92it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:04<00:03,  2.85it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:05<00:02,  3.00it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:05<00:02,  2.91it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:05<00:02,  2.84it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:06<00:01,  3.07it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:06<00:01,  3.25it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:06<00:00,  3.32it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:07<00:00,  3.41it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:07<00:00,  3.67it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A                                                   
                                               [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [2:58:12<00:00, 35.95s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.33it/s][A
                                               [A                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [2:58:12<00:00, 35.95s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [2:58:12<00:00, 35.64s/it]
{'eval_loss': 2.171112060546875, 'eval_runtime': 7.9531, 'eval_samples_per_second': 25.148, 'eval_steps_per_second': 3.143, 'epoch': 112.94}
{'train_runtime': 10692.5162, 'train_samples_per_second': 3.591, 'train_steps_per_second': 0.028, 'train_loss': 0.8987467463811238, 'epoch': 112.94}

###############################################################################
HÃ¡brÃ³k Cluster
Job 3102343 for user 'p306726'
Finished at: Wed Jul 19 13:44:40 CEST 2023

Job details:
============

Job ID              : 3102343
Name                : jobscript.sh
User                : p306726
Partition           : gpushort
Nodes               : a100gpu6
Number of Nodes     : 1
Cores               : 1
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2023-07-19T10:29:45
Start               : 2023-07-19T10:45:26
End                 : 2023-07-19T13:44:40
Reserved walltime   : 04:00:00
Used walltime       : 02:59:14
Used CPU time       : 02:58:11 (efficiency: 99.41%)
% User (Computation): 99.91%
% System (I/O)      :  0.09%
Mem reserved        : 120G
Max Mem (Node/step) : 2.36G (a100gpu6, per node)
Full Max Mem usage  : 2.36G
Total Disk Read     : 724.26M
Total Disk Write    : 857.98M

Acknowledgements:
=================

Please see this page for information about acknowledging HÃ¡brÃ³k in your publications:

https://wiki.hpc.rug.nl/habrok/introduction/scientific_output

################################################################################
